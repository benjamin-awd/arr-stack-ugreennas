#########################
# Centralized Volumes   #
#########################
volumes:
  pihole-etc-pihole:
  pihole-etc-dnsmasq:
  # wireguard-easy-config:
  model-cache:

#########################
# Networks              #
#########################
networks:
  vpn-net:
    driver: bridge
    ipam:
      config:
        - subnet: 10.8.1.0/24
  traefik-proxy:
    external: true

#########################
# Logging               #
#########################
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

#########################
# Services              #
#########################
services:
  #############################################
  # VPN Health Monitor (deunhealth)          #
  #############################################
  deunhealth:
    image: qmcgaw/deunhealth
    container_name: deunhealth
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - TZ=Europe/London
    restart: unless-stopped
    logging: *default-logging

  #############################################
  # VPN Gateway (Gluetun with Surfshark)    #
  #############################################
  gluetun:
    image: qmcgaw/gluetun:v3.40.3
    container_name: gluetun
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    env_file:
      - .env
    volumes:
      - /volume2/docker/arr-stack/gluetun-config:/gluetun
    environment:
      - VPN_SERVICE_PROVIDER=protonvpn
      - VPN_TYPE=wireguard
      - VPN_PORT_FORWARDING=on
      - VPN_PORT_FORWARDING_UP_COMMAND=/bin/sh -c 'wget -O- --retry-connrefused --post-data "json={\"listen_port\":{{PORTS}}}" http://127.0.0.1:8085/api/v2/app/setPreferences 2>&1'
      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY}
      - WIREGUARD_ADDRESSES=${WIREGUARD_ADDRESS}
      - SERVER_COUNTRIES=${VPN_COUNTRIES:-Singapore}
      - TZ=Europe/London
      - DNS_ADDRESS=1.1.1.1
      - FIREWALL_OUTBOUND_SUBNETS=192.168.0.0/24,192.168.100.0/24
    ports:
      # Ports for services using network_mode: "service:gluetun"
      # Enables direct local access without Traefik (http://NAS_IP:PORT)
      - "8989:8989"   # Sonarr
      - "7878:7878"   # Radarr
      - "9696:9696"   # Prowlarr
      - "8085:8085"   # qBittorrent
      - "8191:8191"   # Byparr/FlareSolverr
    network_mode: bridge
    # networks:
    #   traefik-proxy:
    #     ipv4_address: 192.168.100.3
    #   vpn-net:
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "/gluetun-entrypoint", "healthcheck"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s  # Give VPN time to establish connection

  #############################################
  # Download Client (qBittorrent via VPN)    #
  #############################################
  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:5.1.4-r1-ls431
    container_name: qbittorrent
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/London
      - WEBUI_PORT=8085
      - DOCKER_MODS=ghcr.io/vuetorrent/vuetorrent-lsio-mod:2.31.2
    volumes:
      - /volume2/docker/arr-stack/qbittorrent/config:/config
      - /volume1/Media/downloads:/downloads
      - /volume1/Media:/media
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8085/"]
      interval: 1m30s
      timeout: 10s
      retries: 3

  #############################################
  # TV Show Management (Sonarr via VPN)      #
  #############################################
  sonarr:
    image: lscr.io/linuxserver/sonarr:4.0.16.2944-ls299
    container_name: sonarr
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/London
    volumes:
      - /volume2/docker/arr-stack/sonarr/config:/config
      - /volume1/Media/tv:/tv
      - /volume1/Media/downloads:/downloads
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8989/"]
      interval: 2m
      timeout: 10s
      retries: 3

  #############################################
  # Indexer Manager (Prowlarr via VPN)       #
  #############################################
  prowlarr:
    image: lscr.io/linuxserver/prowlarr:2.3.0.5236-ls134
    container_name: prowlarr
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/London
    volumes:
      - /volume2/docker/arr-stack/prowlarr/config:/config
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9696/"]
      interval: 2m
      timeout: 10s
      retries: 3

  #############################################
  # Movie Management (Radarr via VPN)        #
  #############################################
  radarr:
    image: lscr.io/linuxserver/radarr:6.0.4.10291-ls289
    container_name: radarr
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/London
    volumes:
      - /volume2/docker/arr-stack/radarr/config:/config
      - /volume1/Media/movies:/movies
      - /volume1/Media/downloads:/downloads
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7878/"]
      interval: 2m
      timeout: 10s
      retries: 3

  #############################################
  # Media Server (Jellyfin)                  #
  #############################################
  jellyfin:
    image: jellyfin/jellyfin:10.11.5
    container_name: jellyfin
    ports:
      - "8096:8096"  # Local network access
    environment:
      - TZ=Europe/London
    volumes:
      - /volume2/docker/arr-stack/jellyfin/config:/config
      - /volume2/docker/arr-stack/jellyfin/cache:/cache
      - /volume1/Media/movies:/media/movies:ro
      - /volume1/Media/tv:/media/tv:ro
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.4
    labels:
      # Routing handled by traefik/dynamic/vpn-services.yml (not Docker labels)
      - "traefik.enable=true"
      - "traefik.docker.network=traefik-proxy"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8096/health"]
      interval: 3m
      timeout: 10s
      retries: 2

  #############################################
  # DNS & Ad-blocking (Pi-hole)              #
  #############################################
  pihole:
    image: pihole/pihole:2025.11.1
    container_name: pihole
    ports:
      - "5380:53/tcp"  # Use different port to avoid conflict with localhost dnsmasq
      - "5380:53/udp"
      - "8080:80"       # Pi-hole Web UI
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.5
      # vpn-net:
      #   ipv4_address: 10.8.1.200
    environment:
      - TZ=Europe/London
      - FTLCONF_webserver_api_password=${PIHOLE_UI_PASS}
      - FTLCONF_dns_listeningMode=all
    volumes:
      - pihole-etc-pihole:/etc/pihole
      - pihole-etc-dnsmasq:/etc/dnsmasq.d
    labels:
      # Routing handled by traefik/dynamic/vpn-services.yml (not Docker labels)
      - "traefik.enable=true"
      - "traefik.docker.network=traefik-proxy"
    cap_add:
      - NET_ADMIN
      - SYS_NICE
      - SYS_TIME
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "dig", "@127.0.0.1", "-p", "53", "google.com", "+short"]
      interval: 5m
      timeout: 10s
      retries: 2
  
  # using Tailscale so wg-easy isn't needed
  # #############################################
  # # WireGuard VPN Server (wg-easy)           #
  # #############################################
  # wg-easy:
  #   image: ghcr.io/wg-easy/wg-easy
  #   container_name: wg-easy
  #   environment:
  #     - LANG=en
  #     - WG_HOST=wg.${DOMAIN}
  #     - PASSWORD_HASH=${WG_PASSWORD_HASH}
  #     - WG_DEFAULT_ADDRESS=10.8.0.x
  #     - WG_DEFAULT_DNS=10.8.1.200
  #     - TZ=Europe/London
  #   volumes:
  #     - wireguard-easy-config:/etc/wireguard
  #   labels:
  #     # Routing handled by traefik/dynamic/vpn-services.yml (not Docker labels)
  #     - "traefik.enable=true"
  #     - "traefik.docker.network=traefik-proxy"
  #   ports:
  #     - "51820:51820/udp"
  #   networks:
  #       traefik-proxy:
  #         ipv4_address: 192.168.100.6
  #       vpn-net:
  #         ipv4_address: 10.8.1.3
  #   cap_add:
  #     - NET_ADMIN
  #     - SYS_MODULE
  #   sysctls:
  #     - net.ipv4.ip_forward=1
  #     - net.ipv4.conf.all.src_valid_mark=1
  #   restart: unless-stopped
  #   logging: *default-logging
  #   healthcheck:
  #     test: ["CMD", "wg", "show"]
  #     interval: 5m
  #     timeout: 10s
  #     retries: 1

  #############################################
  # Request Manager (Jellyseerr)             #
  #############################################
  jellyseerr:
    image: fallenbagel/jellyseerr:2.7.3
    container_name: jellyseerr
    ports:
      - "5055:5055"  # Local network access
    environment:
      - LOG_LEVEL=info
      - TZ=Europe/London
      - PORT=5055
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.8
    labels:
      # Routing handled by traefik/dynamic/vpn-services.yml (not Docker labels)
      - "traefik.enable=true"
      - "traefik.docker.network=traefik-proxy"
    volumes:
      - /volume2/docker/arr-stack/jellyseerr/config:/app/config
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:5055/api/v1/status"]
      interval: 2m
      timeout: 10s
      retries: 3

  #############################################
  # Subtitle Management (Bazarr)             #
  #############################################
  bazarr:
    image: lscr.io/linuxserver/bazarr:v1.5.3-ls328
    container_name: bazarr
    ports:
      - "6767:6767"  # Local network access
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/London
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.9
    volumes:
      - /volume2/docker/arr-stack/bazarr/config:/config
      - /volume1/Media/movies:/movies
      - /volume1/Media/tv:/tv
    labels:
      # Routing handled by traefik/dynamic/vpn-services.yml (not Docker labels)
      - "traefik.enable=true"
      - "traefik.docker.network=traefik-proxy"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6767/"]
      interval: 3m
      timeout: 10s
      retries: 2

  #############################################
  # Cloudflare Bypass (FlareSolverr)         #
  #############################################
  flaresolverr:
    image: ghcr.io/thephaseless/byparr:b652f3caa15d2c52396f691f3fbec77ff5237609-amd64
    container_name: flaresolverr
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - LOG_LEVEL=info
      - TZ=Europe/London
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8191/"]
      interval: 3m
      timeout: 10s
      retries: 2

  #############################################
  # Monitoring (Uptime Kuma)                 #
  #############################################
  uptime-kuma:
    image: louislam/uptime-kuma:2
    container_name: uptime-kuma
    restart: unless-stopped
    ports:
      - "3001:3001"  # Local network access
    environment:
      - TZ=Europe/London
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.13
    volumes:
      - /volume2/docker/arr-stack/uptime-kuma:/app/data
      - /var/run/docker.sock:/var/run/docker.sock:ro  # For Docker container monitoring
    labels:
      # Routing handled by traefik/dynamic/vpn-services.yml (not Docker labels)
      - "traefik.enable=true"
      - "traefik.docker.network=traefik-proxy"
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "node /app/extra/healthcheck.js"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  #############################################
  # Photo Management (Immich)                #
  #############################################
  database:
    container_name: immich_postgres
    image: ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_USER: ${DB_USERNAME}
      POSTGRES_DB: ${DB_DATABASE_NAME}
      POSTGRES_INITDB_ARGS: '--data-checksums'
    volumes:
      - ${DB_DATA_LOCATION}:/var/lib/postgresql/data
    shm_size: 128mb
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.14
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USERNAME} -d ${DB_DATABASE_NAME}"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 20s

  redis:
    container_name: immich_redis
    image: docker.io/valkey/valkey:8@sha256:81db6d39e1bba3b3ff32bd3a1b19a6d69690f94a3954ec131277b9a26b95b3aa
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.15
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1m
      timeout: 10s
      retries: 3

  immich-server:
    container_name: immich_server
    image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release}
    volumes:
      - ${UPLOAD_LOCATION}:/data
      - ${THUMB_LOCATION}:/data/thumbs
      - ${ENCODED_VIDEO_LOCATION}:/data/encoded-video
      - ${PROFILE_LOCATION}:/data/profile
      - ${BACKUP_LOCATION}:/data/backups
      - /etc/localtime:/etc/localtime:ro
      - /volume1/Media/photos:/mnt/media/photos:ro
    env_file:
      - .env
    ports:
      - "2283:2283"  # Local network access
    depends_on:
      - redis
      - database
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.16
    labels:
      # Routing handled by traefik/dynamic/vpn-services.yml (not Docker labels)
      - "traefik.enable=true"
      - "traefik.docker.network=traefik-proxy"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      disable: false

  immich-machine-learning:
    container_name: immich_machine_learning
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release}
    volumes:
      - model-cache:/cache
    env_file:
      - .env
    networks:
      traefik-proxy:
        ipv4_address: 192.168.100.17
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      disable: false

  #############################################
  # Docker Logs (dozzle)                      #
  #############################################
  dozzle:
    container_name: dozzle
    image: amir20/dozzle:v8.14.12
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 8081:8080
    environment:
      - TZ=Europe/London
    restart: unless-stopped
    logging: *default-logging

  #############################################
  # Wake-on-LAN (Upsnap)                     #
  #############################################
  upsnap:
    container_name: upsnap
    image: ghcr.io/seriousm4x/upsnap:5
    volumes:
      - /volume2/docker/arr-stack/upsnap:/app/pb_data
    network_mode: host
    environment:
      - TZ=Europe/London
      - UPSNAP_INTERVAL=60
      - UPSNAP_SCAN_RANGE=${UPSNAP_SCAN_RANGE:-192.168.0.0/24}
      - UPSNAP_SCAN_TIMEOUT=1000
      - UPSNAP_WEBSITE_TITLE=Upsnap
    restart: unless-stopped
    logging: *default-logging
